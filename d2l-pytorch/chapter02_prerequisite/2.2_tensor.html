<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="/static/img/favicon.png" rel="icon" type="image/png">

    <!-- Theme CSS -->
    <link href="https://freeaihub.oss-cn-beijing.aliyuncs.com/asset/css/theme.css" rel="stylesheet" type="text/css"/>
    <link href="https://freeaihub.oss-cn-beijing.aliyuncs.com/asset/css/style.css" rel="stylesheet" type="text/css"/>
    <title>2.2 数据操作 - FreeAIHub</title>
  
    <style>
      #top_bar {
          /* background-color: #6e84a3;
          color: white;
          font: bold 12px Helvetica;
          padding: 6px 5px 4px 5px;
          border-bottom: 1px outset; */
      }
      #status {
          text-align: center;
      }
      #sendCtrlAltDelButton {
          position: fixed;
          top: 0px;
          right: 0px;
          border: 1px outset;
          padding: 5px 5px 4px 5px;
          cursor: pointer;
      }

      #screen {
          /* flex: 1;
          overflow: hidden; */
      }

  </style>

  </head>
  <body class="bg-light" style="padding-top: 84px;">
      <!-- NAVBAR
    ================================================== -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top bg-white border-bottom">
      <div class="container-fluid" style="height: 42px;">

        <!-- Brand -->
        <a class="navbar-brand" href="../index.html">

          <img src="https://freeaihub.oss-cn-beijing.aliyuncs.com/asset/images/freeaihub.svg" width="60%" alt="freeai logo">
        </a>

        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">

          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>

          <!-- Navigation -->
          <ul class="navbar-nav ml-auto">
         
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#banner">首页</a>
            </li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#banner">课程页面</a>
            </li>
            <li class="nav-item dropdown">
            
            </li>
          </ul>
        </div>

      </div>
    </nav>


    <!-- BREADCRUMB
    ================================================== -->
    <nav class="d-lg-none bg-gray-800">
      <div class="container-fluid">
        <div class="row align-items-center">
          <div class="col">
          </div>
          <div class="col-auto">
            <!-- Toggler -->
            <div class="navbar-dark">
              <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#sidenavCollapse" aria-controls="sidenavCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
              </button>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </nav>

    <!-- CONTENT
    ================================================== -->
    <section style="overflow: hidden;">
      <div class="container-fluid">
        <div class="row">

          <div class="col-12 col-lg-2 col-xl-2 px-lg-0 border-bottom border-bottom-lg-0 border-right-lg border-gray-300 sidenav sidenav-left">     
            <div class="collapse d-lg-block" id="sidenavCollapse">
              <div class="px-lg-5">
                <ul class="nav side-left">
                  
    <li>简介</li>
    <li><a href="/d2l-pytorch/chapter01_DL-intro/deep-learning-intro.html">1. 深度学习简介</a></li>
    <li>预备知识</li>
    <li><a href="/d2l-pytorch/chapter02_prerequisite/2.1_install.html">2.1 环境配置</a></li>
    <li><a href="/d2l-pytorch/chapter02_prerequisite/2.2_tensor.html">2.2 数据操作</a></li>
    <a href="/d2l-pytorch/chapter02_prerequisite/2.3_autograd.html">2.3 自动求梯度</a></li>
    <li>深度学习基础</li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.1_linear-regression.html">3.1 线性回归</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.2_linear-regression-scratch.html">3.2 线性回归的从零开始实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.3_linear-regression-pytorch.html">3.3 线性回归的简洁实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.4_softmax-regression.html">3.4 softmax回归</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.5_fashion-mnist.html">3.5 图像分类数据集（Fashion-MNIST）</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.6_softmax-regression-scratch.html">3.6 softmax回归的从零开始实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.7_softmax-regression-pytorch.html">3.7 softmax回归的简洁实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.8_mlp.html">3.8 多层感知机</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.9_mlp-scratch.html">3.9 多层感知机的从零开始实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.10_mlp-pytorch.html">3.10 多层感知机的简洁实现</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.11_underfit-overfit.html">3.11 模型选择、欠拟合和过拟合</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.12_weight-decay.html">3.12 权重衰减</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.13_dropout.html">3.13 丢弃法</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.14_backprop.html">3.14 正向传播、反向传播和计算图</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.15_numerical-stability-and-init.html">3.15 数值稳定性和模型初始化</a></li>
    <li><a href="/d2l-pytorch/chapter03_DL-basics/3.16_kaggle-house-price.html">3.16 实战Kaggle比赛：房价预测</a></li>
    <li>深度学习计算</li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.1_model-construction.html">4.1 模型构造</a></li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.2_parameters.html">4.2 模型参数的访问、初始化和共享</a></li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.3_deferred-init.html">4.3 模型参数的延后初始化</a></li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.4_custom-layer.html">4.4 自定义层</a></li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.5_read-write.html">4.5 读取和存储</a></li>
    <li><a href="/d2l-pytorch/chapter04_DL_computation/4.6_use-gpu.html">4.6 GPU计算</a></li>
    <li>卷积神经网络</li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.1_conv-layer.html">5.1 二维卷积层</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.2_padding-and-strides.html">5.2 填充和步幅</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.3_channels.html">5.3 多输入通道和多输出通道</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.4_pooling.html">5.4 池化层</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.5_lenet.html">5.5 卷积神经网络（LeNet）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.6_alexnet.html">5.6 深度卷积神经网络（AlexNet）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.7_vgg.html">5.7 使用重复元素的网络（VGG）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.8_nin.html">5.8 网络中的网络（NiN）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.9_googlenet.html">5.9 含并行连结的网络（GoogLeNet）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.10_batch-norm.html">5.10 批量归一化</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.11_resnet.html">5.11 残差网络（ResNet）</a></li>
    <li><a href="/d2l-pytorch/chapter05_CNN/5.12_densenet.html">5.12 稠密连接网络（DenseNet）</a></li>
    <li>循环神经网络</li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.1_lang-model.html">6.1 语言模型</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.2_rnn.html">6.2 循环神经网络</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.3_lang-model-dataset.html">6.3 语言模型数据集（周杰伦专辑歌词）</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.4_rnn-scratch.html">6.4 循环神经网络的从零开始实现</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.5_rnn-pytorch.html">6.5 循环神经网络的简洁实现</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.6_bptt.html">6.6 通过时间反向传播</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.7_gru.html">6.7 门控循环单元（GRU）</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.8_lstm.html">6.8 长短期记忆（LSTM）</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.9_deep-rnn.html">6.9 深度循环神经网络</a></li>
    <li><a href="/d2l-pytorch/chapter06_RNN/6.10_bi-rnn.html">6.10 双向循环神经网络</a></li>
    <li>优化算法</li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.1_optimization-intro.html">7.1 优化与深度学习</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.2_gd-sgd.html">7.2 梯度下降和随机梯度下降</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.3_minibatch-sgd.html">7.3 小批量随机梯度下降</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.4_momentum.html">7.4 动量法</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.5_adagrad.html">7.5 AdaGrad算法</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.6_rmsprop.html">7.6 RMSProp算法</a></li>
    <li><a href="/d2l-pytorch/chapter07_optimization/7.7_adadelta.html">7.7 AdaDelta算法</a></li>
    <a href="/d2l-pytorch/chapter07_optimization/7.8_adam.html">7.8 Adam算法</a></li>
    <li>计算性能</li>
    <li><a href="/d2l-pytorch/chapter08_computational-performance/8.1_hybridize.html">8.1 命令式和符号式混合编程</a></li>
    <li><a href="/d2l-pytorch/chapter08_computational-performance/8.2_async-computation.html">8.2 异步计算</a></li>
    <li><a href="/d2l-pytorch/chapter08_computational-performance/8.3_auto-parallelism.html">8.3 自动并行计算</a></li>
    <li><a href="/d2l-pytorch/chapter08_computational-performance/8.4_multiple-gpus.html">8.4 多GPU计算</a></li>
    <li>计算机视觉</li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.1_image-augmentation.html">9.1 图像增广</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.2_fine-tuning.html">9.2 微调</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.3_bounding-box.html">9.3 目标检测和边界框</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.4_anchor.html">9.4 锚框</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.5_multiscale-object-detection.html">9.5 多尺度目标检测</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.6_object-detection-dataset.html">9.6 目标检测数据集（皮卡丘）</a></li>
    <li>9.7 单发多框检测（SSD）</li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.8_rcnn.html">9.8 区域卷积神经网络（R-CNN）系列</a></li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.9_semantic-segmentation-and-dataset.html">9.9 语义分割和数据集</a></li>
    <li>9.10 全卷积网络（FCN）</li>
    <li><a href="/d2l-pytorch/chapter09_computer-vision/9.11_neural-style.html">9.11 样式迁移</a></li>
    <li>9.12 实战Kaggle比赛：图像分类（CIFAR-10）</li>
    <li>9.13 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）<li>
    <li>自然语言处理</li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.1_word2vec.html">10.1 词嵌入（word2vec）</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.2_approx-training.html">10.2 近似训练</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.3_word2vec-pytorch.html">10.3 word2vec的实现</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.4_fasttext.html">10.4 子词嵌入（fastText）</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.5_glove.html">10.5 全局向量的词嵌入（GloVe）</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.6_similarity-analogy.html">10.6 求近义词和类比词</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.html">10.7 文本情感分类：使用循环神经网络</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.html">10.8 文本情感分类：使用卷积神经网络（textCNN）</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.9_seq2seq.html">10.9 编码器—解码器（seq2seq）</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.10_beam-search.html">10.10 束搜索</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.11_attention.html">10.11 注意力机制</a></li>
    <li><a href="/d2l-pytorch/chapter10_natural-language-processing/10.12_machine-translation.html">10.12 机器翻译</a></li>
                </ul>  

              </div>
            </div>


          </div>

          <div class="entry-cellcontent col-10 col-lg-10 col-xl-10 offset-lg-2 offset-xl-2">
          <h1>2.2 数据操作</h1>
<p>在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。</p>
<p>在PyTorch中，<code>torch.Tensor</code>是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现<code>Tensor</code>和NumPy的多维数组非常类似。然而，<code>Tensor</code>提供GPU计算和自动求梯度等更多功能，这些使<code>Tensor</code>更加适合深度学习。 </p>
<blockquote>
<p>"tensor"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。</p>
</blockquote>
<h2>2.2.1 创建<code>Tensor</code></h2>
<p>我们先介绍<code>Tensor</code>的最基本功能，即<code>Tensor</code>的创建。</p>
<p>首先导入PyTorch：</p>
<pre><code class="python">import torch
</code></pre>

<p>然后我们创建一个5x3的未初始化的<code>Tensor</code>：</p>
<pre><code class="python">x = torch.empty(5, 3)
print(x)
</code></pre>

<p>输出：</p>
<pre><code>tensor([[ 0.0000e+00,  1.5846e+29,  0.0000e+00],
        [ 1.5846e+29,  5.6052e-45,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5846e+29, -2.4336e+02]])
</code></pre>

<p>创建一个5x3的随机初始化的<code>Tensor</code>:</p>
<pre><code class="python">x = torch.rand(5, 3)
print(x)
</code></pre>

<p>输出：</p>
<pre><code>tensor([[0.4963, 0.7682, 0.0885],
        [0.1320, 0.3074, 0.6341],
        [0.4901, 0.8964, 0.4556],
        [0.6323, 0.3489, 0.4017],
        [0.0223, 0.1689, 0.2939]])
</code></pre>

<p>创建一个5x3的long型全0的<code>Tensor</code>:</p>
<pre><code class="python">x = torch.zeros(5, 3, dtype=torch.long)
print(x)
</code></pre>

<p>输出：</p>
<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
</code></pre>

<p>还可以直接根据数据创建:</p>
<pre><code class="python">x = torch.tensor([5.5, 3])
print(x)
</code></pre>

<p>输出：</p>
<pre><code>tensor([5.5000, 3.0000])
</code></pre>

<p>还可以通过现有的<code>Tensor</code>来创建，此方法会默认重用输入<code>Tensor</code>的一些属性，例如数据类型，除非自定义数据类型。</p>
<pre><code class="python">x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device
print(x)

x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型
print(x) 
</code></pre>

<p>输出：</p>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 0.6035,  0.8110, -0.0451],
        [ 0.8797,  1.0482, -0.0445],
        [-0.7229,  2.8663, -0.5655],
        [ 0.1604, -0.0254,  1.0739],
        [ 2.2628, -0.9175, -0.2251]])
</code></pre>

<p>我们可以通过<code>shape</code>或者<code>size()</code>来获取<code>Tensor</code>的形状:</p>
<pre><code class="python">print(x.size())
print(x.shape)
</code></pre>

<p>输出：</p>
<pre><code>torch.Size([5, 3])
torch.Size([5, 3])
</code></pre>

<blockquote>
<p>注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。</p>
</blockquote>
<p>还有很多函数可以创建<code>Tensor</code>，去翻翻官方API就知道了，下表给了一些常用的作参考。</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Tensor(*sizes)</td>
<td align="center">基础构造函数</td>
</tr>
<tr>
<td align="center">tensor(data,)</td>
<td align="center">类似np.array的构造函数</td>
</tr>
<tr>
<td align="center">ones(*sizes)</td>
<td align="center">全1Tensor</td>
</tr>
<tr>
<td align="center">zeros(*sizes)</td>
<td align="center">全0Tensor</td>
</tr>
<tr>
<td align="center">eye(*sizes)</td>
<td align="center">对角线为1，其他为0</td>
</tr>
<tr>
<td align="center">arange(s,e,step)</td>
<td align="center">从s到e，步长为step</td>
</tr>
<tr>
<td align="center">linspace(s,e,steps)</td>
<td align="center">从s到e，均匀切分成steps份</td>
</tr>
<tr>
<td align="center">rand/randn(*sizes)</td>
<td align="center">均匀/标准分布</td>
</tr>
<tr>
<td align="center">normal(mean,std)/uniform(from,to)</td>
<td align="center">正态分布/均匀分布</td>
</tr>
<tr>
<td align="center">randperm(m)</td>
<td align="center">随机排列</td>
</tr>
</tbody>
</table>
<p>这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。</p>
<h2>2.2.2 操作</h2>
<p>本小节介绍<code>Tensor</code>的各种操作。</p>
<h3>算术操作</h3>
<p>在PyTorch中，同一种操作可能有很多种形式，下面用加法作为例子。
* <strong>加法形式一</strong>
    <code>python
    y = torch.rand(5, 3)
    print(x + y)</code>
* <strong>加法形式二</strong>
    <code>python
    print(torch.add(x, y))</code>
    还可指定输出：
    <code>python
    result = torch.empty(5, 3)
    torch.add(x, y, out=result)
    print(result)</code>
* <strong>加法形式三、inplace</strong>
    <code>python
    # adds x to y
    y.add_(x)
    print(y)</code>
    &gt; <strong>注：PyTorch操作inplace版本都有后缀<code>_</code>, 例如<code>x.copy_(y), x.t_()</code></strong></p>
<p>以上几种形式的输出均为：</p>
<pre><code>tensor([[ 1.3967,  1.0892,  0.4369],
        [ 1.6995,  2.0453,  0.6539],
        [-0.1553,  3.7016, -0.3599],
        [ 0.7536,  0.0870,  1.2274],
        [ 2.5046, -0.1913,  0.4760]])
</code></pre>

<h3>索引</h3>
<p>我们还可以使用类似NumPy的索引操作来访问<code>Tensor</code>的一部分，需要注意的是：<strong>索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。</strong> </p>
<pre><code class="python">y = x[0, :]
y += 1
print(y)
print(x[0, :]) # 源tensor也被改了
</code></pre>

<p>输出：</p>
<pre><code>tensor([1.6035, 1.8110, 0.9549])
tensor([1.6035, 1.8110, 0.9549])
</code></pre>

<p>除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">index_select(input, dim, index)</td>
<td align="center">在指定维度dim上选取，比如选取某些行、某些列</td>
</tr>
<tr>
<td align="center">masked_select(input, mask)</td>
<td align="center">例子如上，a[a&gt;0]，使用ByteTensor进行选取</td>
</tr>
<tr>
<td align="center">nonzero(input)</td>
<td align="center">非0元素的下标</td>
</tr>
<tr>
<td align="center">gather(input, dim, index)</td>
<td align="center">根据index，在dim维度上选取数据，输出的size与index一样</td>
</tr>
</tbody>
</table>
<p>这里不详细介绍，用到了再查官方文档。</p>
<h3>改变形状</h3>
<p>用<code>view()</code>来改变<code>Tensor</code>的形状：</p>
<pre><code class="python">y = x.view(15)
z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来
print(x.size(), y.size(), z.size())
</code></pre>

<p>输出：</p>
<pre><code>torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])
</code></pre>

<p><strong>注意<code>view()</code>返回的新<code>Tensor</code>与源<code>Tensor</code>虽然可能有不同的<code>size</code>，但是是共享<code>data</code>的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)</strong></p>
<pre><code class="python">x += 1
print(x)
print(y) # 也加了1
</code></pre>

<p>输出：</p>
<pre><code>tensor([[1.6035, 1.8110, 0.9549],
        [1.8797, 2.0482, 0.9555],
        [0.2771, 3.8663, 0.4345],
        [1.1604, 0.9746, 2.0739],
        [3.2628, 0.0825, 0.7749]])
tensor([1.6035, 1.8110, 0.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,
        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])
</code></pre>

<p>所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个<code>reshape()</code>可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用<code>clone</code>创造一个副本然后再使用<code>view</code>。<a href="https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch">参考此处</a></p>
<pre><code class="python">x_cp = x.clone().view(15)
x -= 1
print(x)
print(x_cp)
</code></pre>

<p>输出:</p>
<pre><code>tensor([[ 0.6035,  0.8110, -0.0451],
        [ 0.8797,  1.0482, -0.0445],
        [-0.7229,  2.8663, -0.5655],
        [ 0.1604, -0.0254,  1.0739],
        [ 2.2628, -0.9175, -0.2251]])
tensor([1.6035, 1.8110, 0.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,
        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])
</code></pre>

<blockquote>
<p>使用<code>clone</code>还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源<code>Tensor</code>。</p>
</blockquote>
<p>另外一个常用的函数就是<code>item()</code>, 它可以将一个标量<code>Tensor</code>转换成一个Python number：</p>
<pre><code class="python">x = torch.randn(1)
print(x)
print(x.item())
</code></pre>

<p>输出：</p>
<pre><code>tensor([2.3466])
2.3466382026672363
</code></pre>

<h3>线性代数</h3>
<p>另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">trace</td>
<td align="center">对角线元素之和(矩阵的迹)</td>
</tr>
<tr>
<td align="center">diag</td>
<td align="center">对角线元素</td>
</tr>
<tr>
<td align="center">triu/tril</td>
<td align="center">矩阵的上三角/下三角，可指定偏移量</td>
</tr>
<tr>
<td align="center">mm/bmm</td>
<td align="center">矩阵乘法，batch的矩阵乘法</td>
</tr>
<tr>
<td align="center">addmm/addbmm/addmv/addr/baddbmm..</td>
<td align="center">矩阵运算</td>
</tr>
<tr>
<td align="center">t</td>
<td align="center">转置</td>
</tr>
<tr>
<td align="center">dot/cross</td>
<td align="center">内积/外积</td>
</tr>
<tr>
<td align="center">inverse</td>
<td align="center">求逆矩阵</td>
</tr>
<tr>
<td align="center">svd</td>
<td align="center">奇异值分解</td>
</tr>
</tbody>
</table>
<p>PyTorch中的<code>Tensor</code>支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考<a href="https://pytorch.org/docs/stable/tensors.html">官方文档</a>。</p>
<h2>2.2.3 广播机制</h2>
<p>前面我们看到如何对两个形状相同的<code>Tensor</code>做按元素运算。当对两个形状不同的<code>Tensor</code>按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个<code>Tensor</code>形状相同后再按元素运算。例如：</p>
<pre><code class="python">x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)
</code></pre>

<p>输出：</p>
<pre><code>tensor([[1, 2]])
tensor([[1],
        [2],
        [3]])
tensor([[2, 3],
        [3, 4],
        [4, 5]])
</code></pre>

<p>由于<code>x</code>和<code>y</code>分别是1行2列和3行1列的矩阵，如果要计算<code>x + y</code>，那么<code>x</code>中第一行的2个元素被广播（复制）到了第二行和第三行，而<code>y</code>中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。</p>
<h2>2.2.4 运算的内存开销</h2>
<p>前面说了，索引操作是不会开辟新内存的，而像<code>y = x + y</code>这样的运算是会新开内存的，然后将<code>y</code>指向新内存。为了演示这一点，我们可以使用Python自带的<code>id</code>函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。</p>
<pre><code class="python">x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y = y + x
print(id(y) == id_before) # False 
</code></pre>

<p>如果想指定结果到原来的<code>y</code>的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把<code>x + y</code>的结果通过<code>[:]</code>写进<code>y</code>对应的内存中。</p>
<pre><code class="python">x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y[:] = y + x
print(id(y) == id_before) # True
</code></pre>

<p>我们还可以使用运算符全名函数中的<code>out</code>参数或者自加运算符<code>+=</code>(也即<code>add_()</code>)达到上述效果，例如<code>torch.add(x, y, out=y)</code>和<code>y += x</code>(<code>y.add_(x)</code>)。</p>
<pre><code class="python">x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
torch.add(x, y, out=y) # y += x, y.add_(x)
print(id(y) == id_before) # True
</code></pre>

<blockquote>
<p>注：虽然<code>view</code>返回的<code>Tensor</code>与源<code>Tensor</code>是共享<code>data</code>的，但是依然是一个新的<code>Tensor</code>（因为<code>Tensor</code>除了包含<code>data</code>外还有一些其他属性），二者id（内存地址）并不一致。</p>
</blockquote>
<h2>2.2.5 <code>Tensor</code>和NumPy相互转换</h2>
<p>我们很容易用<code>numpy()</code>和<code>from_numpy()</code>将<code>Tensor</code>和NumPy中的数组相互转换。但是需要注意的一点是：
<strong>这两个函数所产生的的<code>Tensor</code>和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！</strong></p>
<blockquote>
<p>还有一个常用的将NumPy中的array转换成<code>Tensor</code>的方法就是<code>torch.tensor()</code>, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的<code>Tensor</code>和原来的数据不再共享内存。</p>
</blockquote>
<h3><code>Tensor</code>转NumPy</h3>
<p>使用<code>numpy()</code>将<code>Tensor</code>转换成NumPy数组:</p>
<pre><code class="python">a = torch.ones(5)
b = a.numpy()
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)
</code></pre>

<p>输出：</p>
<pre><code>tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]
tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]
tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]
</code></pre>

<h3>NumPy数组转<code>Tensor</code></h3>
<p>使用<code>from_numpy()</code>将NumPy数组转换成<code>Tensor</code>:</p>
<pre><code class="python">import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)
</code></pre>

<p>输出：</p>
<pre><code>[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)
</code></pre>

<p>所有在CPU上的<code>Tensor</code>（除了<code>CharTensor</code>）都支持与NumPy数组相互转换。</p>
<p>此外上面提到还有一个常用的方法就是直接用<code>torch.tensor()</code>将NumPy数组转换成<code>Tensor</code>，需要注意的是该方法总是会进行数据拷贝，返回的<code>Tensor</code>和原来的数据不再共享内存。</p>
<pre><code class="python">c = torch.tensor(a)
a += 1
print(a, c)
</code></pre>

<p>输出</p>
<pre><code>[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)
</code></pre>

<h2>2.2.6 <code>Tensor</code> on GPU</h2>
<p>用方法<code>to()</code>可以将<code>Tensor</code>在CPU和GPU（需要硬件支持）之间相互移动。</p>
<pre><code class="python"># 以下代码只有在PyTorch GPU版本上才会执行
if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)          # GPU
    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor
    x = x.to(device)                       # 等价于 .to(&quot;cuda&quot;)
    z = x + y
    print(z)
    print(z.to(&quot;cpu&quot;, torch.double))       # to()还可以同时更改数据类型
</code></pre>

<hr />
<blockquote>
<p>注: 本文主要参考<a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py">PyTorch官方文档</a>和<a href="https://github.com/chenyuntc/pytorch-book/blob/master/chapter3-Tensor%E5%92%8Cautograd/Tensor.ipynb">此处</a>，与<a href="https://zh.d2l.ai/chapter_prerequisite/ndarray.html">原书同一节</a>有很大不同。</p>
</blockquote>
          </div>
          <backend type="k"></backend>
          <code class=gatsby-kernelname data-language=python></code>
        </div> <!-- / .row -->
      </div>
      
    </section>

    <!-- JAVASCRIPT
    ================================================== -->
    <!-- Libs JS -->
    <script src="https://landkit.goodthemes.co/assets/libs/jquery/dist/jquery.min.js"></script>
    <script src="https://landkit.goodthemes.co/assets/libs/bootstrap/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Theme JS -->
    <script src="https://landkit.goodthemes.co/assets/js/theme.min.js"></script>
    <script src="https://cdn.freeaihub.com/asset/js/cell.js"></script>
          
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </body>
</html>